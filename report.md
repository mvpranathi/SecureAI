# ðŸ›¡ Secure AI Systems â€“ Red & Blue Teaming MNIST Classifier  
**Team Member:** *MV Pranathi*  
**GitHub Repository:** https://github.com/mvpranathi/SecureAI  

---

## ðŸ“Œ 1. Introduction  
This project demonstrates a **complete AI security pipeline** where a CNN model is trained to classify MNIST digits, then attacked using **data poisoning**, analyzed for security vulnerabilities, and defended using **adversarial training**.

The goal is to understand **how to secure AI models using red teaming (attacking)** and **blue teaming (defending)** approaches.

---

## ðŸ§  2. Model Training (Baseline Performance)

The model used: **Convolutional Neural Network (CNN)**  
Trained on **clean MNIST dataset**.

| Metric  | Value |
|---------|------|
| Accuracy | **98.45%** |
| Loss | **0.0402** |
| Inference Time | < 20ms per image |

ðŸ“Œ Confusion matrix & training details â†’ `report/confusion_matrix.md`

---

## âš  3. STRIDE Threat Model (Security Analysis)

| STRIDE Category | Threat in AI System |
|----------------|---------------------|
| **S**poofing | Fake/adversarial inputs |
| **T**ampering | Data poisoning attacks |
| **R**epudiation | No tracking of attack origin |
| **I**nformation Disclosure | Model parameters may leak |
| **D**enial of Service | Overloading model with inputs |
| **E**levation of Privilege | Manipulating inputs to force misclassification |

âž¡ Full details in `report/stride_threat_model.md`

---

## ðŸ” 4. SAST (Security Testing) using Bandit

Command used:
```bash
bandit -r .

Result:
âœ” No major security vulnerabilities detected.

ðŸ“„ Full scan report â†’ report/sast_report.txt

ðŸ”´ 5. Red Team Attack â€“ Data Poisoning (Method 1 ONLY)

Attack Method:

Selected 100 samples of digit â€˜7â€™

Added a small red square in corner â†’ poisoned images

Retrained CNN on poisoned dataset

ðŸ“Œ Code in adversarial/poison_dataset.py & outputs saved in poisoned_images.npy

ðŸ“‰ Attack Results (After Poisoning)
Metric	Value
Accuracy	99.11%
Loss	0.0314

ðŸ“„ Report file â†’ report/adversarial_attack_results.md

ðŸ›¡ 6. Blue Team Defense â€“ Adversarial Training

Defense Strategy:
Trained model using clean + poisoned data
This improves robustness against data poisoning attacks.

ðŸ§¾ Results (Defended Model)
Metric	Value
Accuracy	98.76%
Loss	0.0482

ðŸ“„ Report file â†’ report/defense_results.md

ðŸ“Š 7. Performance Comparison
Model Type	Accuracy	Loss
Clean Model	98.45%	0.0402
Poisoned Model	99.11%	0.0314
Defended Model	98.76%	0.0482

ðŸ“Œ Comparison table saved as â†’ report/performance_comparison.md

ðŸ§¾ 8. Summary of Work

âœ” Built CNN for MNIST digit recognition
âœ” Applied STRIDE threat model
âœ” Performed security scan using Bandit (SAST)
âœ” Conducted data poisoning attack
âœ” Defended model using adversarial training
âœ” Compared baseline, attacked, and defended models

ðŸ“„ Consolidated summary â†’ report/final_summary.md

ðŸ 9. Conclusion

This assignment helped understand AI system security beyond accuracy.
We successfully demonstrated:

âœ” Model training
âœ” Attack simulation (Red Team)
âœ” Defense strategy (Blue Team)
âœ” Security analysis using STRIDE & SAST

âž¡ Security must be an integral part of AI development, not an afterthought.

ðŸ“„ Conclusion â†’ report/conclusion.md
