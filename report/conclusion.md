# ðŸ§  Final Conclusion â€“ Secure AI Systems (MNIST Red & Blue Teaming)

The goal of the assignment was to:
âœ” Train a CNN model on MNIST dataset  
âœ” Perform **Red Team attack (data poisoning)**  
âœ” Perform **Blue Team defense (adversarial training)**  
âœ” Compare performance metrics and document security posture  

---

## ðŸ”´ Red Team â€“ Attack Summary
- We introduced poisoned samples by adding a white 3Ã—3 pixel marker.
- Model performance remained high, but certain inputs with triggers caused misclassification.
- This proves **CNNs are vulnerable to data poisoning** if no defense is used.

---

## ðŸ”µ Blue Team â€“ Defense Summary
- Defense strategy: **Adversarial Training + Clean Data**
- Re-trained model using both datasets â†’ model became more robust.
- Final performance:  
  - âœ” 98.77% Accuracy (Defended Model)  
  - â— Slightly lower than clean model but **much more secure**

---

## ðŸ“Œ Learning Outcomes
| Concept | Learning Gained |
|--------|------------------|
| Data Poisoning | Understanding of subtle model manipulation |
| STRIDE | Cybersecurity approach for AI systems |
| Adversarial Defense | How to improve model robustness |
| SAST Analysis | Code scanning for vulnerabilities |
| GitHub Repo | Version control for AI projects |

---

## ðŸ“˜ Future Work
- Implement API security (JWT authentication)
- Encrypt stored model files
- Improve logging & monitoring
- Try **FGSM / PGD adversarial attack generation**

---

### ðŸ Final Statement  
> This assignment showed that **AI models are not only about accuracy** â€”  
> âœ” **Security is equally important.**  
> âœ” **Adversarial defense improves trustworthiness** of AI systems.  
> âœ” **Model robustness > high accuracy**