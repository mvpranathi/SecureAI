# STRIDE Threat Model â€“ MNIST Classifier

| STRIDE Category | Threat Example | Risk Level | Possible Mitigation |
|-----------------|----------------|------------|----------------------|
| **S â€“ Spoofing** | Fake input pretending to be another user/class | Medium | Input validation, authentication |
| **T â€“ Tampering** | Data poisoning (changing labels or pixels) | ðŸ”¥ HIGH | Data integrity check, hashing |
| **R â€“ Repudiation** | No logs of who trained the model | Low | Logging + audit trails |
| **I â€“ Information Disclosure** | Model leak exposes patterns | Medium | Secure model storage |
| **D â€“ Denial of Service** | Large batch crashes training | Medium | Rate limiting, batch size restriction |
| **E â€“ Elevation of Privilege** | Bypass model security layers | ðŸ”¥ HIGH | Role-based access control |

---

### ðŸ§  Final Observation
The MNIST classifier is **vulnerable to data poisoning attacks**,  
but **defense training improves robustness** significantly.

To make the system secure in production:
- Use **adversarial training regularly**
- Enable **data validation**
- Keep **secure logging + versioning**
- Deploy model via **API with authentication**
